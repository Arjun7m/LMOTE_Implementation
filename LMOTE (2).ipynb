{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiNn5SKV35oM"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSBjvDqK4pBr"
      },
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/V1.4_Training.csv')\n",
        "print(train_df.shape)\n",
        "train_df.dropna(inplace=True)\n",
        "print(train_df.shape)\n",
        "train_df.head()\n",
        "nonpol_samples = []\n",
        "#sugg_samples = train_df.loc[train_df['Tag']==1]['Sentence']\n",
        "#sugg_samples[0]\n",
        "for i in range(len(train_df)):\n",
        "    if(train_df.iloc[i]['Tag']==1):\n",
        "        nonpol_samples.append(train_df.loc[i][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2JrQl7D5n8F"
      },
      "source": [
        "nonpol_samples[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PpprJH65oYs"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english')) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kpHjkwS5rR_"
      },
      "source": [
        "textp = ' '.join(nonpol_samples)\n",
        "textp = textp.lower()\n",
        "textp = textp.translate(textp.maketrans('','', string.punctuation))\n",
        "textp = word_tokenize(textp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjV8HkNA5tzs"
      },
      "source": [
        "ngrams = lambda a, n: zip(*[a[i:] for i in range(n)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkG1P66h5wzf"
      },
      "source": [
        "from collections import Counter\n",
        "mc_np = Counter(ngrams(textp, 5)).most_common(4330)\n",
        "mc_np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj1yYxgM5yc_"
      },
      "source": [
        "mc_less_np = Counter(ngrams(textp, 5)).most_common(187)\n",
        "mc_less_np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2-wMWht51H1"
      },
      "source": [
        "def preprocess(df):\n",
        "    inp = df.values\n",
        "    for i in range(len(inp)):\n",
        "        s = inp[i]\n",
        "        sl = s.lower()\n",
        "        sl = result = sl.translate(sl.maketrans('','', string.punctuation))\n",
        "        tokens = word_tokenize(sl)\n",
        "        #tokens = [w for w in tokens if not w in stop_words]\n",
        "        inp[i] = tokens\n",
        "    return inp\n",
        "\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/V1.4_Training.csv')\n",
        "train_df.dropna(inplace=True)\n",
        "train_df = train_df[train_df['Tag']==1]['Sentence']\n",
        "textall = preprocess(train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVD-UUvs6Eok"
      },
      "source": [
        "import numpy as np\n",
        "en_model = {}\n",
        "f = open('/content/drive/My Drive/data/glove.6B.50d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    en_model[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(en_model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah9ZpQCgKrsf"
      },
      "source": [
        "for t in textall:\n",
        "    for w in t:\n",
        "        if w in string.punctuation:\n",
        "            t.remove(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swXNWx0DLT6n"
      },
      "source": [
        "print(type(textall))\n",
        "print(textall.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YfReiERLWIQ"
      },
      "source": [
        "words=[]\n",
        "keys=[]\n",
        "i=1\n",
        "print('starting')\n",
        "for k, tsent in enumerate(textall):\n",
        "    if (k%1000 == 0):\n",
        "        print ('k', k, k/textall.size)\n",
        "    for j, word in enumerate(tsent):\n",
        "        #if (j%100 == 0):\n",
        "            #print ('j', j)\n",
        "        if word not in words:\n",
        "            words.append(word)\n",
        "            keys.append(i)\n",
        "            i=i+1\n",
        "print ('done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZlmJmGTLYe2"
      },
      "source": [
        "len(words), len(keys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y_b9RyeLt8W"
      },
      "source": [
        "embed_len = len(words)\n",
        "print (embed_len)\n",
        "\n",
        "word2id = dict(zip(words, keys))\n",
        "id2word = dict(zip(keys, words))\n",
        "embeddings = np.zeros((embed_len+1,len(en_model['test'])))  # num_words * 50 (word vec len)\n",
        "print('starting')\n",
        "for i, w in enumerate(words):\n",
        "    try:\n",
        "        vec = en_model[w]\n",
        "    except KeyError:\n",
        "        pass\n",
        "    embeddings[word2id[w]]=vec\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPACs3a2L2Of"
      },
      "source": [
        "def ngrams_lang_inp(arr):\n",
        "    inp = []\n",
        "    curr = []\n",
        "    for s in arr:\n",
        "        i=0\n",
        "        while(i+4<len(s)):\n",
        "            ngram = list(s[i:i+5])\n",
        "            idngram = [word2id[w] for w in ngram]\n",
        "            if i+5<len(s):\n",
        "                nextword = word2id[s[i+5]]   \n",
        "            else:\n",
        "                break\n",
        "            curr = [idngram, nextword]\n",
        "            inp.append(curr)\n",
        "            curr = []\n",
        "            i=i+1\n",
        "        \n",
        "    return inp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvZK-xpLMGNd"
      },
      "source": [
        "npol_text = []\n",
        "nonpol_len = len(nonpol_samples)\n",
        "print (nonpol_len)\n",
        "print ('starting')\n",
        "for i in range(nonpol_len):\n",
        "    s = nonpol_samples[i]\n",
        "    sl = s.lower()\n",
        "    sl = result = sl.translate(sl.maketrans('','', string.punctuation))\n",
        "    tokens = word_tokenize(sl)\n",
        "    npol_text.append(tokens)\n",
        "print ('finishing')\n",
        "\n",
        "npol_lang_seq = ngrams_lang_inp(npol_text)\n",
        "train_xp = np.array([s[0] for s in npol_lang_seq])\n",
        "train_yp = np.array([s[1] for s in npol_lang_seq])\n",
        "\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXZiIJtlMIwu"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, Input, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "modelp = Sequential()\n",
        "#model.add(Input(shape =(4,) , dtype='int32'))\n",
        "modelp.add(Embedding(embeddings.shape[0], output_dim = 50,\n",
        "                     weights = [embeddings], input_length=5, trainable = True))\n",
        "modelp.add(Bidirectional(LSTM(256, activation = 'relu')))\n",
        "modelp.add(Dense(1024, activation = 'relu'))\n",
        "modelp.add(Dense(embeddings.shape[0], activation = 'softmax'))\n",
        "\n",
        "modelp.compile(optimizer = 'Adam',\n",
        "               loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2JyO-WdMK0C"
      },
      "source": [
        "modelp.fit(train_xp, train_yp, batch_size = 32, epochs=20, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwkzFYwIMrGg"
      },
      "source": [
        "modelp.save('/content/drive/My Drive/data/LMOTE_model_SM')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zms_n8HBcbPm"
      },
      "source": [
        "nonpol_generated = []\n",
        "for np in mc_np:\n",
        "    ngram_gen = np[0] #('', '', '', '')\n",
        "    idngram = []\n",
        "    for p in ngram_gen:\n",
        "        idngram.append(word2id[p])\n",
        "    #[1, 2, 3, 4]\n",
        "    ran_len = np.random.randint(70, 100)\n",
        "    sentence = [id2word[i] for i in idngram]\n",
        "    curr = idngram[:4]\n",
        "    for l in range(ran_len):\n",
        "        inp = np.array([curr, [2, 34, 5, 6]])\n",
        "        p = np.argmax(modelp.predict(inp)[0])\n",
        "        wordp = id2word[p]\n",
        "        sentence.append(wordp)\n",
        "        curr = np.delete(curr, 0)\n",
        "        curr = np.append(curr, p)\n",
        "    nonpol_generated.append(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBcpm58Eh8IT"
      },
      "source": [
        "less_nonpol_generated = []\n",
        "\n",
        "for lnp in mc_less_np:\n",
        "    ngram_gen = lnp[0] #('', '', '', '')\n",
        "    idngram = []\n",
        "    for p in ngram_gen:\n",
        "        idngram.append(word2id[p])\n",
        "    #[1, 2, 3, 4]\n",
        "    ran_len = np.random.randint(30, 40)\n",
        "    sentence = [id2word[i] for i in idngram]\n",
        "    curr = idngram[:4]\n",
        "    for l in range(ran_len):\n",
        "        inp = np.array([curr, [2, 34, 5, 6]])\n",
        "        p = np.argmax(modelp.predict(inp)[0])\n",
        "        wordp = id2word[p]\n",
        "        sentence.append(wordp)\n",
        "        curr = np.delete(curr, 0)\n",
        "        curr = np.append(curr, p)\n",
        "        \n",
        "    npint = np.random.randint(0, len(mc_np))\n",
        "    npseed = mc_pos[npint][0]\n",
        "    npseed_idgram = [word2id[p] for p in npseed]\n",
        "    ran_len_np = np.random.randint(20, 30)\n",
        "    \n",
        "    sentence_nonpol = [id2word[i] for i in npseed_idgram]\n",
        "    currnp = npseed_idgram[:4]\n",
        "    for l in range(ran_len_np):\n",
        "        inp = np.array([currnp, [2, 34, 5, 6]])\n",
        "        np = np.argmax(modelp.predict(inp)[0])\n",
        "        wordnp = id2word[p]\n",
        "        sentence_pos.append(wordnp)\n",
        "        currnp = np.delete(currnp, 0)\n",
        "        currnp = np.append(currnp, np)\n",
        "    sentence = sentence + sentence_nonpol\n",
        "    less_nonpol_generated.append(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlh71u4hiAX0"
      },
      "source": [
        "' '.join(less_nonpol_generated[100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwuJdKFNiE7B"
      },
      "source": [
        "all_gen_sentences = []\n",
        "for s in less_nonpol_generated:\n",
        "    all_gen_sentences.append(' '.join(s))\n",
        "\n",
        "for s in nonpol_generated:\n",
        "    all_gen_sentences.append(' '.join(s))\n",
        "    \n",
        "gen_df = pd.DataFrame(all_gen_sentences)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkeuzxwjiJwi"
      },
      "source": [
        "gen_df.to_csv('non_pol.csv')\n",
        "!cp data.csv \"drive/My Drive/data/\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}